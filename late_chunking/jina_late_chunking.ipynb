{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c0d766-85a6-454f-9ab2-90e5547d3f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T07:49:51.533986Z",
     "iopub.status.busy": "2024-12-20T07:49:51.533309Z",
     "iopub.status.idle": "2024-12-20T07:49:59.146056Z",
     "shell.execute_reply": "2024-12-20T07:49:59.145190Z",
     "shell.execute_reply.started": "2024-12-20T07:49:51.533947Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/envs/myenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/admin/anaconda3/envs/myenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169bf5d1-be7e-469f-bafc-1ec4798b9f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T07:50:02.398716Z",
     "iopub.status.busy": "2024-12-20T07:50:02.396824Z",
     "iopub.status.idle": "2024-12-20T07:50:02.410548Z",
     "shell.execute_reply": "2024-12-20T07:50:02.409485Z",
     "shell.execute_reply.started": "2024-12-20T07:50:02.398658Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer\n",
    "    :param input_text: The text snippet to split into sentences\n",
    "    :param tokenizer: The tokenizer to use\n",
    "    :return: A tuple containing the list of text chunks and their corresponding token spans\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8df4a0-fdf2-440d-85a8-36ae8baa2332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T07:50:04.159084Z",
     "iopub.status.busy": "2024-12-20T07:50:04.158659Z",
     "iopub.status.idle": "2024-12-20T07:50:04.178358Z",
     "shell.execute_reply": "2024-12-20T07:50:04.177899Z",
     "shell.execute_reply.started": "2024-12-20T07:50:04.159056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks:\n",
      "- \"Berlin is the capital and largest city of Germany, both by area and by population.\"\n",
      "- \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"\n",
      "- \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
    "\n",
    "# determine chunks\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)\n",
    "print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9fd8be-c56d-4899-a151-c04ae7465b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T07:50:05.531642Z",
     "iopub.status.busy": "2024-12-20T07:50:05.531191Z",
     "iopub.status.idle": "2024-12-20T07:50:05.539772Z",
     "shell.execute_reply": "2024-12-20T07:50:05.539244Z",
     "shell.execute_reply.started": "2024-12-20T07:50:05.531617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 17), (17, 44), (44, 69)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d504a44f-f37a-4320-9b33-80f51a37af12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T07:50:10.826082Z",
     "iopub.status.busy": "2024-12-20T07:50:10.825511Z",
     "iopub.status.idle": "2024-12-20T07:50:10.833907Z",
     "shell.execute_reply": "2024-12-20T07:50:10.832825Z",
     "shell.execute_reply.started": "2024-12-20T07:50:10.826047Z"
    }
   },
   "outputs": [],
   "source": [
    "def late_chunking(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ab34fe-f9b5-4008-bd45-de955a6ef091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T07:51:21.313752Z",
     "iopub.status.busy": "2024-12-20T07:51:21.313378Z",
     "iopub.status.idle": "2024-12-20T07:51:21.325033Z",
     "shell.execute_reply": "2024-12-20T07:51:21.324582Z",
     "shell.execute_reply.started": "2024-12-20T07:51:21.313733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 27])\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    chunk_inputs = tokenizer(chunk, return_tensors='pt')\n",
    "    print(chunk_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f9e6383-d405-433c-bcde-4b3284bdda1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T07:06:43.534430Z",
     "iopub.status.busy": "2024-12-20T07:06:43.532929Z",
     "iopub.status.idle": "2024-12-20T07:06:43.827910Z",
     "shell.execute_reply": "2024-12-20T07:06:43.827462Z",
     "shell.execute_reply.started": "2024-12-20T07:06:43.534359Z"
    }
   },
   "outputs": [],
   "source": [
    "# chunk before\n",
    "embeddings_traditional_chunking = model.encode(chunks)\n",
    "\n",
    "# chunk afterwards (context-sensitive chunked pooling)\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "model_output = model(**inputs)\n",
    "embeddings = late_chunking(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc36542-d4a2-4a65-9e24-c94bb8bf34a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T07:12:29.021556Z",
     "iopub.status.busy": "2024-12-20T07:12:29.021219Z",
     "iopub.status.idle": "2024-12-20T07:12:29.026161Z",
     "shell.execute_reply": "2024-12-20T07:12:29.025549Z",
     "shell.execute_reply.started": "2024-12-20T07:12:29.021534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_traditional_chunking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46585814-afdf-4c0c-8dc0-6717b1b01ac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T07:07:00.802624Z",
     "iopub.status.busy": "2024-12-20T07:07:00.802016Z",
     "iopub.status.idle": "2024-12-20T07:07:00.876205Z",
     "shell.execute_reply": "2024-12-20T07:07:00.875734Z",
     "shell.execute_reply.started": "2024-12-20T07:07:00.802587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_new(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.849546\n",
      "similarity_trad(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.8486218\n",
      "similarity_new(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.82489026\n",
      "similarity_trad(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.7084338\n",
      "similarity_new(\"Berlin\", \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"): 0.84980094\n",
      "similarity_trad(\"Berlin\", \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"): 0.75345534\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "berlin_embedding = model.encode('Berlin')\n",
    "\n",
    "for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "    print(f'similarity_new(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, new_embedding))\n",
    "    print(f'similarity_trad(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, trad_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a56afa-0c6f-4f4c-a294-bbfa0a79ccb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
